{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with linear Conditional Random Fields \n",
    "\n",
    "This assignment is comprised of two parts:\n",
    "\n",
    "1. **Theory**: Solve two exercises about POS tagging and run Viterbi for a simple HMM model.\n",
    "2. **Implementation**: Experiment with a name entity recognizer using sequence tagging with CRFs.\n",
    "\n",
    "For the implementation part, you will be using the `python-crfsuite` that can be installed using Anaconda as follows: `conda install -c conda-forge python-crfsuite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your Name Here: Shreyas Lokesha\n",
    "##### UNC-id: 801210964\n",
    "##### e-mail: slokesha@uncc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *crf-ner.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *crf-ner.ipynb*.\n",
    "6. The PDF file may not show the table with the Viterbi solution. Make sure that you submit the notebook file as well!\n",
    "7. Submit **both** your PDF and notebook on Canvas. Also upload the **CRF output** on the testb blind test set in CoNLL format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: POS tagging exercises\n",
    "\n",
    "1. Exercise 8.1 in Chapter 8 in J&M.\n",
    "2. Exercise 8.2 in Chapter 8 in J&M, parts 1, 2, 3, and 4.\n",
    "\n",
    "To solve these exercises, it will be useful to consult the POS tagging guidelines linked on the course website, as well as the POS tagging examples provided in the files under `data/wsj/23/*.pos`. <br>\n",
    "\n",
    "(1)\n",
    "1. I/PRP need/VBP a/DT flight/NN from/IN Atlanta/NN <br>\n",
    "Tagging Error Rectified: Atlanta/NNP (Proper Noun) <br>\n",
    "\n",
    "2. Does/VBZ this/DT flight/NN serve/VB dinner/NNS<br>\n",
    "Tagging Error Rectified: Dinner/NN (Singular Noun) <br>\n",
    "\n",
    "3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP<br>\n",
    "Tagging Error Rectified: have/VBP (3rd person singular present) <br>\n",
    "\n",
    "4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN flights/NNS <br>\n",
    "Tagging Error Rectified: Can/MD<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "(2)\n",
    "1. It is a nice night.<br>\n",
    "Tagged Sequence: It/PRP is/VBZ a/DT nice/JJ night/NN ./PUNC<br>\n",
    "\n",
    "2. This crap game is over a garage in Fifty-second Street. . .<br>\n",
    "Tagged Sequence: This/DT crap/NN game/NN is/VBZ over/IN a/DT garage/NN in/IN Fifty-Second/NNP Street/NNP ./PUNC ./PUNC ./PUNC <br>\n",
    "\n",
    "3.  . . . Nobody ever takes the newspapers she sells . . . <br>\n",
    "Tagged Sequence:./PUNC ./PUNC ./PUNC Nobody/NN ever/RB takes/VBZ the/DT newspapers/NNS she/PRP sells/VBZ ./PUNC \n",
    "./PUNC ./PUNC <br>\n",
    "\n",
    "4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a mournful voice. <br>\n",
    "Tagged Sequence: He/PRP is/VBZ a/DT tall/JJ ,/PUNC skinny/JJ guy/NN with/IN a/DT long/JJ ,/PUNC sad/JJ ,/PUNC \n",
    "mean-looking/JJ kisser/NN ,/PUNC and/CC a/DT mournful/JJ voice/NN ./PUNC <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Run Viterbi for a simple HMM model\n",
    "\n",
    "Consider the following parameters for a very simple HMM:\n",
    "\n",
    "<img src=\"files/table1.png\" width=\"350\">\n",
    "\n",
    "<img src=\"files/table2.png\" width=\"250\">\n",
    "\n",
    "Create the trellis for the sentence \"Fed cuts rates in half\".\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th align=\"left\">State/Word</th>\n",
    "    <th>(t = 1) Fed</th>\n",
    "    <th>(t = 2) cuts</th>\n",
    "    <th>(t = 3) rates</th>\n",
    "    <th>(t = 4) in</th>\n",
    "    <th>(t = 5) half</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">$S_1$ = Noun</td>\n",
    "    <td>0.5 * 0.1 / 0</td>\n",
    "    <td>0.0144 / 2</td>\n",
    "    <td>0.0027 / 2</td>\n",
    "    <td>0 / 0</td>\n",
    "    <td>0.000020738 / 3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">$S_2$ = Verb</td>\n",
    "    <td>0.3 * 0.2 / 0</td>\n",
    "    <td>0.015 / 1</td>\n",
    "    <td>0.00288 / 1</td>\n",
    "    <td>0 / 0</td>\n",
    "    <td>0 / 0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\">$S_3$ = Prep</td>\n",
    "    <td>0.2 * 0.0 / 0</td>\n",
    "    <td>0 / 0</td>\n",
    "    <td>0 / 0</td>\n",
    "    <td>0.0002304 / 2</td>\n",
    "    <td>0 / 0</td>\n",
    "  </tr>\n",
    "    <caption>Trellis table, showing the $\\delta_i(t)$ scores for each state $S_i$ at step $t$.</caption>\n",
    "</table>\n",
    "\n",
    "1. Show the computed parameters $\\delta_i(t)$ / $\\psi_i(t)$ at each node in the trellis. \n",
    "2. What is the most likely sequence of tags? What is its probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "$\\delta_1(1) = 0.5 x 0.2 = 0.10$\n",
    "\n",
    "$\\delta_2(1) = 0.3 x 0.2 = 0.06$\n",
    "\n",
    "$\\delta_3(1) = 0.2 x 0.0 = 0$\n",
    "\n",
    "\n",
    "$\\delta_1(2) = max(0.10 x 0.2 x 0.4, 0.06 x 0.6 x 0.4, 0 x 0.9 x 0.4) = max(0.008, 0.0144, 0) = 0.0144 => \\psi_1(2) = 2 $\n",
    "\n",
    "$\\delta_2(2) = max(0.10 x 0.5 x 0.3, 0.06 x 0.0 x 0.3, 0 x 0.1 x 0.3) = max(0.015, 0, 0) = 0.015 => \\psi_2(2) = 1 $\n",
    "\n",
    "$\\delta_3(2) = max(0.10 x 0.3 x 0.0, 0.06 x 0.4 x 0.0, 0 x 0.0 x 0.0) = max(0, 0, 0) = 0 => \\psi_3(2) = 0 $\n",
    "\n",
    "\n",
    "$\\delta_1(3) = max(0.0144 x 0.2 x 0.3, 0.015 x 0.6 x 0.3, 0 x 0.9 x 0.3) = max(0.00086, 0.0027, 0) = 0.0027 => \\psi_1(3) = 2$\n",
    "\n",
    "$\\delta_2(3) = max(0.0144 x 0.5 x 0.4, 0.015 x 0.0 x 0.4, 0 x 0.1 x 0.4) = max(0.00288, 0, 0) = 0.00288 => \\psi_2(3) = 1$\n",
    "\n",
    "$\\delta_3(3) = max(0.0144 x 0.3 x 0, 0.015 x 0.4 x 0, 0 x 0 x 0) = max(0, 0, 0) = 0 => \\psi_3(3) = 0$\n",
    "\n",
    "\n",
    "$\\delta_1(4) = max(0.0027 x 0.2 x 0, 0.00288 x 0.6 x 0, 0 x 0.9 x 0.2) = max(0, 0, 0) = 0 => \\psi_1(4) = 0$\n",
    "\n",
    "$\\delta_2(4) = max(0.0027 x 0.5 x 0, 0.00288 x 0.0 x 0, 0 x 0.1 x 0.2) = max(0, 0, 0) = 0 => \\psi_2(4) = 0$\n",
    "\n",
    "$\\delta_3(4) = max(0.0027 x 0.3 x 0.2, 0.00288 x 0.4 x 0.2, 0 x 0 x 0.2) = max(0.00016, 0.0002304, 0) = 0.0002304 => \\psi_3(4) = 2$\n",
    "\n",
    "\n",
    "$\\delta_1(5) = max(0 x 0.2 x 0.1, 0 x 0.6 x 0.1, 0.0002304 x 0.9 x 0.1) = max(0, 0, 0.000020738) = 0.000020738 => \\psi_1(5) = 3$\n",
    "\n",
    "$\\delta_2(5) = max(0 x 0.5 x 0, 0 x 0 x 0, 0.0002304 x 0.1 x 0) = max(0, 0, 0) = 0 => \\psi_2(5) = 0$\n",
    "\n",
    "$\\delta_3(5) = max(0 x 0.3 x 0, 0 x 0.2 x 0, 0.0002304 x 0 x 0) = max(0, 0, 0) = 0 => \\psi_3(5) = 0$\n",
    "\n",
    "\n",
    "2. Probabilistically determined Sequence of Tags:\n",
    "\n",
    "Verb -> Noun -> Verb -> Prep -> Noun  <br>\n",
    "\n",
    "Probabilistic value  : 0.000020738 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Named entity recognition is the task of identifying references to named entities of certain types in text. We\n",
    "use data presented in the CoNLL 2003 Shared Task (Tjong Kim Sang and De Meulder, 2003). An example\n",
    "of the data is given below:\n",
    "\n",
    "    Singapore NNP I-NP B-ORG\n",
    "    Refining NNP I-NP I-ORG\n",
    "    Company NNP I-NP I-ORG\n",
    "    expected VBD I-VP O\n",
    "    to TO I-VP O\n",
    "    shut VB I-VP O\n",
    "    CDU NNP I-NP B-ORG\n",
    "    3 CD I-NP I-ORG\n",
    "    . . O NONE O\n",
    "    \n",
    "There are four columns here: the word, the POS tag, the chunk bit (a form of shallow parsing—you can ignore this), and the column containing the NER tag. NER labels are given in a BIO tag scheme: beginning, inside, outside. In the example above, two named entities are present: Singapore Refining Company and CDU 3. O tags denote text not part of a named entity. B tags indicate the start of a named entity, and I tags indicate the continuation of the previous named entity. Both B and I tags are hyphenated and contain a type after the hyphen, which in this dataset is one of PER, ORG, LOC, or MISC. A B tag can immediately follow another B tag in the case where a one-word entity is followed immediately by another entity. However, note that an I tag can only follow an I tag or B tag of the same type.\n",
    "\n",
    "A NER system’s job is to predict the NER chunks of an unseen sentence, i.e., predict the last column given the others. Output is typically evaluated according to chunk-level F-measure. To evaluate a single\n",
    "sentence, let C denote the predicted set of labeled chunks represented by a tuple of (label, start index, end\n",
    "index) and let C* denote the gold set of chunks. We compute precision (P), recall (R), and F1 as follows:\n",
    "\n",
    "$P = \\displaystyle\\frac{|C \\cap C^*|}{|C|}$; $R = \\displaystyle\\frac{|C \\cap C^*|}{|C^*|}$; $F_1 = \\displaystyle\\frac{2PR}{(P+R)}$\n",
    "\n",
    "The gold labeled chunks from the example above are (ORG, 0, 3) and (ORG, 6, 8) using 0-based indexing\n",
    "and semi-inclusive notation for intervals.\n",
    "\n",
    "To generalize to corpus-level evaluation, the numerators and denominators of precision and recall are aggregated across the corpus. State-of-the-art systems can get above 90 F1 on this dataset; we’ll be aiming to get close to this and build systems that can get in at least the mid-80s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nerdata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1n/3021sx6n2gn99kbcsy7kgsfm0000gn/T/ipykernel_955/3944789625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnerdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabeledSentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_from_bio_tag_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nerdata'"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nerdata import Token, Chunk, LabeledSentence, chunks_from_bio_tag_seq, read_data, print_evaluation, print_output\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of sentences:\n",
      "['Token(CRICKET, NNP, I-NP)', 'Token(-, :, O)', 'Token(LEICESTERSHIRE, NNP, I-NP)', 'Token(TAKE, NNP, I-NP)', 'Token(OVER, IN, I-PP)', 'Token(AT, NNP, I-NP)', 'Token(TOP, NNP, I-NP)', 'Token(AFTER, NNP, I-NP)', 'Token(INNINGS, NNP, I-NP)', 'Token(VICTORY, NN, I-NP)', 'Token(., ., O)']\n",
      "['(2, 3, ORG)']\n",
      "['Token(West, NNP, I-NP)', 'Token(Indian, NNP, I-NP)', 'Token(all-rounder, NN, I-NP)', 'Token(Phil, NNP, I-NP)', 'Token(Simmons, NNP, I-NP)', 'Token(took, VBD, I-VP)', 'Token(four, CD, I-NP)', 'Token(for, IN, I-PP)', 'Token(38, CD, I-NP)', 'Token(on, IN, I-PP)', 'Token(Friday, NNP, I-NP)', 'Token(as, IN, I-PP)', 'Token(Leicestershire, NNP, I-NP)', 'Token(beat, VBD, I-VP)', 'Token(Somerset, NNP, I-NP)', 'Token(by, IN, I-PP)', 'Token(an, DT, I-NP)', 'Token(innings, NN, I-NP)', 'Token(and, CC, O)', 'Token(39, CD, I-NP)', 'Token(runs, NNS, I-NP)', 'Token(in, IN, I-PP)', 'Token(two, CD, I-NP)', 'Token(days, NNS, I-NP)', 'Token(to, TO, I-VP)', 'Token(take, VB, I-VP)', 'Token(over, IN, I-PP)', 'Token(at, IN, B-PP)', 'Token(the, DT, I-NP)', 'Token(head, NN, I-NP)', 'Token(of, IN, I-PP)', 'Token(the, DT, I-NP)', 'Token(county, NN, I-NP)', 'Token(championship, NN, I-NP)', 'Token(., ., O)']\n",
      "['(0, 2, MISC)', '(3, 5, PER)', '(12, 13, ORG)', '(14, 15, ORG)']\n",
      "['Token(After, IN, I-PP)', 'Token(bowling, VBG, I-NP)', 'Token(Somerset, NNP, I-NP)', 'Token(out, RP, I-PRT)', 'Token(for, IN, I-PP)', 'Token(83, CD, I-NP)', 'Token(on, IN, I-PP)', 'Token(the, DT, I-NP)', 'Token(opening, NN, I-NP)', 'Token(morning, NN, I-NP)', 'Token(at, IN, I-PP)', 'Token(Grace, NNP, I-NP)', 'Token(Road, NNP, I-NP)', 'Token(,, ,, O)', 'Token(Leicestershire, NNP, I-NP)', 'Token(extended, VBD, I-VP)', 'Token(their, PRP$, I-NP)', 'Token(first, JJ, I-NP)', 'Token(innings, NN, I-NP)', 'Token(by, IN, I-PP)', 'Token(94, CD, I-NP)', 'Token(runs, VBZ, I-VP)', 'Token(before, IN, I-PP)', 'Token(being, VBG, I-VP)', 'Token(bowled, VBD, I-VP)', 'Token(out, RP, I-PRT)', 'Token(for, IN, I-PP)', 'Token(296, CD, I-NP)', 'Token(with, IN, I-PP)', 'Token(England, NNP, I-NP)', 'Token(discard, VBP, I-VP)', 'Token(Andy, NNP, I-NP)', 'Token(Caddick, NNP, I-NP)', 'Token(taking, VBG, I-VP)', 'Token(three, CD, I-NP)', 'Token(for, IN, I-PP)', 'Token(83, CD, I-NP)', 'Token(., ., O)']\n",
      "['(2, 3, ORG)', '(11, 13, LOC)', '(14, 15, ORG)', '(29, 30, LOC)', '(31, 33, PER)']\n"
     ]
    }
   ],
   "source": [
    "# Load the training and development data.\n",
    "train = read_data('../data/eng.train')\n",
    "dev = read_data('../data/eng.testa')\n",
    "#stopList = open('../data/stopWordlist.txt', 'r', encoding = \"utf-8\").split(\"\\n\")\n",
    "stopList = set()\n",
    "with open ('../data/stopWordlist.txt', 'r', encoding = \"utf-8\") as file:\n",
    "    for line in file:\n",
    "        stopList.add(line.strip('\\n'))\n",
    "        \n",
    "                     \n",
    "# Here's a few sentences...\n",
    "print(\"Examples of sentences:\")\n",
    "print(dev[1])\n",
    "print(dev[3])\n",
    "print(dev[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Next, define some features. To get full credit on the assignment, you should get a score of at least 85% F1 on the development set. Assignments falling short of this will be judged based on completeness and awarded partial credit accordingly. The instructors’ reference implementation was able to get over 88% F1, see if you can beat that!\n",
    "\n",
    "In this example we use word identity, word suffix, word shape and word POS tag; also, some information from nearby words is used.\n",
    "\n",
    "This makes a simple baseline, but you certainly can add and remove some features to get (much?) better results - experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_features(sent, i):\n",
    "    features = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    word = sent.tokens[i].word\n",
    "    postag = sent.tokens[i].pos\n",
    "    features = [\n",
    "        'word.isNamedEnt=' + str(neClassifier(word)),\n",
    "        'word.posTag=' + postag,\n",
    "        'word.isNum=' + str(word.isdigit()),\n",
    "        'word.isAlpha=' + str(word.isalpha()),\n",
    "        'word.wordLen=' + str(len(word))\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent.tokens[i - 1].word\n",
    "        postag1 = sent.tokens[i-1].pos\n",
    "        features.extend([\n",
    "            '-1:word.isNamedEnt=' + str(neClassifier(word1)),\n",
    "            '-1:word.posTag=' + postag1,\n",
    "            '-1:word.isNum=' + str(word1.isdigit()),\n",
    "            '-1:word.isAlpha=' + str(word1.isalpha()),\n",
    "            '-1:word.wordLen=' + str(len(word1))\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent.tokens[i + 1].word\n",
    "        postag1 = sent.tokens[i + 1].pos\n",
    "        features.extend([\n",
    "            '+1:word.isNamedEnt=' + str(neClassifier(word1)),\n",
    "            '+1:word.posTag=' + postag1,\n",
    "            '+1:word.isNum=' + str(word1.isdigit()),\n",
    "            '+1:word.isAlpha=' + str(word1.isalpha()),\n",
    "            '+1:word.wordLen=' + str(len(word1))\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS') \n",
    "            \n",
    "        \n",
    "    return features\n",
    "\n",
    "def neClassifier(word):\n",
    "   \n",
    "    if((word.lower() not in stopList) and word[0].isupper()):\n",
    "        neTag = True\n",
    "    else:\n",
    "        neTag = False\n",
    "\n",
    "    \n",
    "    return neTag\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent.tokens[i].word\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent.tokens[i - 1].word\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent.tokens[i + 1].word\n",
    "        postag1 = sent.tokens[i + 1].pos\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [my_features(sent, i) + word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return sent.bio_tags\n",
    "\n",
    "def sent2words(sent):\n",
    "    return [token.word for token in sent.tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a sample of features created with `word2features()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word.isNamedEnt=False',\n",
       " 'word.posTag=IN',\n",
       " 'word.isNum=False',\n",
       " 'word.isAlpha=True',\n",
       " 'word.wordLen=5',\n",
       " 'BOS',\n",
       " '+1:word.isNamedEnt=False',\n",
       " '+1:word.posTag=VBG',\n",
       " '+1:word.isNum=False',\n",
       " '+1:word.isAlpha=True',\n",
       " '+1:word.wordLen=7',\n",
       " 'bias',\n",
       " 'word.lower=after',\n",
       " 'word.isupper=False',\n",
       " 'BOS',\n",
       " '+1:word.lower=bowling',\n",
       " '+1:word.isupper=False']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(dev[5])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map training and development sentences to their feature sets, one set of features for each position in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train]\n",
    "y_train = [sent2labels(s) for s in train]\n",
    "\n",
    "X_dev = [sent2features(s) for s in dev]\n",
    "y_dev = [sent2labels(s) for s in dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the CRF model\n",
    "\n",
    "To train the model, we create `pycrfsuite.Trainer`, load the training data and call the `train()` method.\n",
    "\n",
    "First, create `pycrfsuite.Trainer` and load the training data into the trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose = False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the training parameters. We will use the default L-BFGS training algorithm with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This displays a list of all the possible parameters for the default training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature.minfreq',\n",
       " 'feature.possible_states',\n",
       " 'feature.possible_transitions',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'max_iterations',\n",
       " 'num_memories',\n",
       " 'epsilon',\n",
       " 'period',\n",
       " 'delta',\n",
       " 'linesearch',\n",
       " 'max_linesearch']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the CRF model and save it into a file in the current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('conll2002-eng.crfmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pycrfsuite.Trainer.train` saves model to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 3A5C-23CD\n",
      "\n",
      " Directory of C:\\Users\\dell\\Documents\\PythonScripts\\JupyterNB\\hw07\\code\n",
      "\n",
      "29-11-2021  15:53           384,000 conll2002-eng.crfmodel\n",
      "               1 File(s)        384,000 bytes\n",
      "               0 Dir(s)  120,762,679,296 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir conll2002-eng.crfmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get information about the final state of the model by looking at the `pycrfsuite.Trainer.logparser`.\n",
    "\n",
    "If we had tagged our input data using the optional group argument in add, and had used the optional holdout argument during train, there would be information about the trainer's performance on the holdout set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num': 50,\n",
       " 'scores': {},\n",
       " 'loss': 14462.099812,\n",
       " 'feature_norm': 119.144445,\n",
       " 'error_norm': 554.819342,\n",
       " 'active_features': 6351,\n",
       " 'linesearch_trials': 1,\n",
       " 'linesearch_step': 1.0,\n",
       " 'time': 0.201}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logparser.last_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get this information for every step using `pycrfsuite.Trainer.logparser.iterations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " {'num': 50,\n",
       "  'scores': {},\n",
       "  'loss': 14462.099812,\n",
       "  'feature_norm': 119.144445,\n",
       "  'error_norm': 554.819342,\n",
       "  'active_features': 6351,\n",
       "  'linesearch_trials': 1,\n",
       "  'linesearch_step': 1.0,\n",
       "  'time': 0.201})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainer.logparser.iterations), trainer.logparser.iterations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on a sample sentence\n",
    "\n",
    "To use the trained model, create a `pycrfsuite.Tagger` object, load the model into it, and use the `tag` method. Let's tag a sentence to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
      "\n",
      "Predicted: O O B-ORG O O O O O O O O B-LOC I-LOC O B-ORG O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O\n",
      "Correct:   O O B-ORG O O O O O O O O B-LOC I-LOC O B-ORG O O O O O O O O O O O O O O B-LOC O B-PER I-PER O O O O O\n"
     ]
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002-eng.crfmodel')\n",
    "\n",
    "example_sent = dev[5]\n",
    "print(' '.join(sent2words(example_sent)), end = '\\n\\n')\n",
    "\n",
    "print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n",
    "print(\"Correct:  \", ' '.join(sent2labels(example_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF model evaluation on development data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict entity labels for all sentences in the  development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 575 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the predicted BIO token-level tags into NE label chunks and compare against the gold NE labels to compute Precision, Recall, and F1 measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled F1: 86.09, precision: 5037/5759 = 87.46, recall: 5037/5943 = 84.76\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "for i, sent in enumerate(dev):\n",
    "    pred.append(LabeledSentence(sent.tokens, chunks_from_bio_tag_seq(y_pred[i])))\n",
    "\n",
    "print_evaluation(dev, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-level evaluation of NE recognizer\n",
    "\n",
    "Implement a function `fine_grained_evaluation(gold, pred)` that computes precision, recall, and F1 measure for each of the 4 name entity types (LOC, MISC, ORG, PER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1n/3021sx6n2gn99kbcsy7kgsfm0000gn/T/ipykernel_955/228057104.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfine_grained_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLabeledSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLabeledSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'LOC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MISC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ORG'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PER'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     value = {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def fine_grained_evaluation(gold: List[LabeledSentence], pred: List[LabeledSentence]):\n",
    "    results = {'LOC': (0, 0, 0), 'MISC': (0, 0, 0), 'ORG': (0, 0, 0), 'PER': (0, 0, 0), }\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    value = {\n",
    "        'LOC' :{\n",
    "            'correct':0,\n",
    "            'predNum':0,\n",
    "            '' : 0 \n",
    "        },\n",
    "        'MISC' :{\n",
    "            'correct':0,\n",
    "            'predNum':0,\n",
    "            'correctTag' : 0 \n",
    "        },\n",
    "        'ORG' :{\n",
    "            'correct':0,\n",
    "            'predNum':0,\n",
    "            'correctTag' : 0 \n",
    "        },\n",
    "        'PER' :{\n",
    "            'correct':0,\n",
    "            'predNum':0,\n",
    "            'correctTag' : 0 \n",
    "         }\n",
    "    }\n",
    "    \n",
    "    for g,p in zip(gold,pred):\n",
    "        for i in range(len(p.chunks)):\n",
    "            if (p.chunks[i]!=[]):\n",
    "                p_tag = p.chunks[i].label\n",
    "                if(p.chunks[i] in g.chunks):\n",
    "                    value[p_tag]['goldNum'] += 1\n",
    "                    value[p_tag]['predNum'] += 1\n",
    "                    value[p_tag]['correctTag'] += 1\n",
    "                else:\n",
    "                    value[p_tag]['predNum'] += 1\n",
    "        for x in range(len(g.chunks)):\n",
    "            if(g.chunks[x] != []):\n",
    "                g_tag = g.chunks[x].label\n",
    "                if(g.chunks[x] not in p.chunks):\n",
    "                    value[g_tag]['goldNum'] += 1\n",
    "    for x,y in value.items():\n",
    "        if(y['predNum'] == 0):\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = y['correctTag']/float(y['predNum'])\n",
    "        if(y['goldNum'] == 0):\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = y['correctTag']/float(y['goldNum'])\n",
    "        if(y['goldNum'] == 0 and y['predNum'] == 0):\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2 * precision * recall)/float(precision + recall)\n",
    "        results[x] = (precision * 100, recall * 100, f1 * 100)\n",
    "        \n",
    "  \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1n/3021sx6n2gn99kbcsy7kgsfm0000gn/T/ipykernel_955/1510127191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mopeval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLabeledSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLabeledSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msetx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msetx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def opeval(gold: List[LabeledSentence], pred: List[LabeledSentence]):\n",
    "    setx = zip(gold,pred)\n",
    "    for a,b in setx:\n",
    "        print(a.chunks)\n",
    "        print(b.chunks)\n",
    "        \n",
    "opeval(dev, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on LOC is: P = 88.61439312567131 ; R = 89.82035928143712 ; F1 = 89.213300892133\n",
      "Performance on MISC is: P = 87.63769889840881 ; R = 77.57313109425785 ; F1 = 82.29885057471263\n",
      "Performance on ORG is: P = 84.50244698205547 ; R = 77.25577926920208 ; F1 = 80.71679002726918\n",
      "Performance on PER is: P = 88.18770226537217 ; R = 88.76221498371335 ; F1 = 88.47402597402598\n"
     ]
    }
   ],
   "source": [
    "results = fine_grained_evaluation(dev, pred)\n",
    "for ne_type in results:\n",
    "    p, r, f1 = results[ne_type]\n",
    "    print('Performance on', ne_type, 'is: P =', p, '; R =', r, '; F1 =', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output on blind test data\n",
    "\n",
    "Once you are done with feature engineering on development data, run the trained CRF model on the blind test data and save the output into a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote predictions on 3684 labeled sentences to ../data/eng.testb.out\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = read_data('../data/eng.testb.blind')\n",
    "X_test = [sent2features(s) for s in test]\n",
    "\n",
    "y_pred = [tagger.tag(xseq) for xseq in X_test]\n",
    "pred = []\n",
    "for i, sent in enumerate(test):\n",
    "    pred.append(LabeledSentence(sent.tokens, chunks_from_bio_tag_seq(y_pred[i])))\n",
    "\n",
    "print_output(pred, '../data/eng.testb.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5111] Comparison Conditional Random Fields vs. Logistic Regression\n",
    "\n",
    "Train and evaluate a NE recognizer on the same dataset using Logistic Regression. Use the same features as in the CRF model. Compare the performance of the two models and explain any observed difference in performance. It is recommended that you do this in a separate notebook file called *lr-ner.ipynb*, with the output saved as PDF in *lr-ner.pdf*.\n",
    "\n",
    "*This portion is mandatory for graduate students. Undergraduate students who implement this will get a substantial number of bonus points*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "Anything extra goes here. For example:\n",
    "\n",
    "- **Features** Try features on POS tags and other improvements to the feature set. If you do this, you should do more than just add some a few new features—add detailed quantitative analysis and cite some examples showing what helps and what doesn’t.\n",
    "\n",
    "- **German** You also have access to German NER data—does the system perform well on this data? Can you add features or change it to get better performance? Note that simply running your model on this dataset and reporting results does not constitute a substantial extension.\n",
    "\n",
    "- **Sentence boundaries** The \"invalid tag sequence\" warnings are caused by I tags predicted at the beginning of sentences. One way of helping the system learn that sentences do not start with an I tag is to pad each sentence with a special `<beg>` token at the beginning and `<end>` token at the end, both tagged as O. Implement this data transformation and train and evaluate the CRF model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "Include an analysis of the results that you obtained in the experiments above. Present results from both the basic CRF model as well as with different features and system variants as part of your extension, and optionally discuss error cases addressed by your extension or describe how the system could be further improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
