{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Sentiment analysis \n",
    "\n",
    "This assignment is comprised of two parts:\n",
    "\n",
    "1. **Theory**: Solve the Naive Bayes exercises 4.1 and 4.2 from Chapter 4 in the J&M textbook. Reformulate NB to emphasize title words.\n",
    "2. **Implementation**: You will implement and experiment with various feature engineering techniques in the context of Naive Bayes models for Sentiment classification of movie reviews.\n",
    "\n",
    "We will use the NB model implemented in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your Name Here: Shreyas Lokesha\n",
    "##### UNC-ID: 801210964\n",
    "##### e-mail: slokesha@uncc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *SentimentAnalysis.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *SentimentAnalysis.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: J&M Exercise 4.1 ##\n",
    "Your solution goes here ... \n",
    "Since it is given that the prior probabilites for each of the classes are equal <br>\n",
    "P(Positive) = P(Negative) = 1/5 <br>\n",
    "Let P(Sentence|Positive) denote the probability of the sentence being classified as a positive statement. <br>\n",
    "Let P(Sentence|Negative) denote the probability of the sentence being classified as a negative statement. <br>\n",
    "<br>\n",
    "P1 = P(Positive).P(Sentence|Positive) = (1/5) x (9x7x29x4x8)/(100^5)   = 0.00000117 <br>\n",
    "P2 = P(Negative).P(Sentence|Negative) = (1/5) x (16x6x6x15x11)/(100^5) = 0.0000019008 <br>\n",
    "argmax(P1,P2) = P2. <br>\n",
    "\n",
    "Therefore the class is negative <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: J&M Exercise 4.2 ##\n",
    "Your solution goes here ... <br>\n",
    "1. fun, couple, love,love **comedy** <br>\n",
    "2. fast, furious, shoot **action** <br>\n",
    "3. couple, fly, fast, fun, fun **comedy** <br>\n",
    "4. furious, shoot, shoot, fun **action** <br>\n",
    "5. fly, fast, shoot, love **action** <br>\n",
    "<br>\n",
    "Given document: fast, couple, shoot, fly\n",
    "<br>\n",
    "Bag of words = {fun, couple, love, fast, shoot, furious, fly} <br>\n",
    "Cardinality of the set = 7 <br>\n",
    "For computing P(Class) or P(C), we shall follow these steps: <br>\n",
    "P(c) = Nc(Number of classes)/Ndoc(Number of Documents) <br>\n",
    "P(action) = Naction/Ndoc = 3/5. Similarly, P(comedy) = Ncomedy/Ndoc = 2/5 <br>\n",
    "PHat = argmax[c belongs to C] P(c).Product(P(wi|c)) <br>\n",
    "Number of words in documents belonging to class action: 11\n",
    "<br>\n",
    "<br>\n",
    "P(wi|action) = (count(wi,action)+1)/(Sum(count(w,action))+|V|) <br>\n",
    "P(fast|action) = (2+1)/(11+7) = 3/18 <br>\n",
    "P(couple|action) = (0+1)/(11+7) = 1/18 <br>\n",
    "P(shoot|action) = (4+1)/(11+7) = 5/18 <br>\n",
    "P(fly|action) = (1+1)/(11+7) = 2/18 <br>\n",
    "<br>\n",
    "Therefore, P(action|Sentence) = P(action).Product(P(wi|action)) <br>\n",
    "P1 = P(action|Sentence) = (3/5)x(3x1x5x2/(18^4)) = 0.00017146 <br>\n",
    "<br>\n",
    "Number of words in documents belonging to class comedy: 9\n",
    "<br>\n",
    "<br>\n",
    "P(wi|comedy) = (count(wi,comedy)+1)/(Sum(count(w,comedy))+|V|) <br>\n",
    "P(fast|comedy) = (1+1)/(9+7) = 2/16 <br>\n",
    "P(couple|comedy) = (2+1)/(9+7) = 3/16 <br>\n",
    "P(shoot|comedy) = (0+1)/(9+7) = 1/16 <br>\n",
    "P(fly|comedy) = (1+1)/(9+7) = 2/16 <br>\n",
    "<br>\n",
    "Therefore, P(comedy|Sentence) = P(comedy).Product(P(wi|comedy)) <br>\n",
    "P2 = P(comedy|Sentence) = (2/5)x(2x3x1x2/(16^4)) = 0.0001464 <br>\n",
    "<br>\n",
    "PHat = argmax(P1,P2) = 0.00017146 <br>\n",
    "Therefore, the class of the document is action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: Title is $K$ times more important than Body\n",
    "*Mandatory for graduate students, optional for undergraduate students*\n",
    "\n",
    "The Naive Bayes algorithm for text categorization presented in class treats all sections of a document equally, ignoring the fact that words in the title are often more important than words in the text in determining the document category. Modify the Naive Bayes training algorithm to reflect that word occurrences in the title are $K$ times more important than word occurrences in the rest of the document for deciding the class, where $K$ is an input parameter. Describe the idea in English and include pseudocode, akin to the training pseudocode shown in class.\n",
    "\n",
    "Your solution goes here ...\n",
    "<br>\n",
    "Let us assume that K is a said weight added to each of the word tokens appearing in the title. <br>\n",
    "Now, if we are to include the k weighted counts of occurences of words in the title along with processing step of the document, we get <br>\n",
    "PHat(title&document) = argmax(c belongs to C) P(c).(count(wi|c)+count(wj|c)+l)/(Sum(count(w|c))+|V|.l+|x|.l) <br>\n",
    "Where wj belongs to word token set of words in the titles and x is the vocabulary formed by the words in the title and c is a sub-class in a big category C<br>\n",
    "<br>\n",
    "<br>\n",
    "Since |x| << |V| as we can not expect title to be bigger than the document itself,<br>\n",
    "we can take |V| + |x| = |V|.<br>\n",
    "To enumerate count(wi|c), we know that, it contains words that potentially may be present in the title too <br>\n",
    "Therefore, wj is potentialy a subset of wi <br>\n",
    "PHat(title&document) = P(c).K.(count(wi|c)+l/K)/(Sum(count(w|c))+|V|.l/K) (Applying laplace smoothing coefficient l/k)<br>\n",
    "PHat(title&document) = P(c).K.PHat(document) <br>\n",
    "<br>\n",
    "<br>\n",
    "Although, the factor will not be exactly K because, of the estimations made, the factor would be approximately = K.\n",
    "<br>\n",
    "<br>\n",
    "Pseudocode:<br>\n",
    "extract word features in title, store it in title_vocab\n",
    "extract word features in document, store it in document_vocab\n",
    "func computePriorAndLikelihood() <br>\n",
    "Prior[c] = Nc/(Ndoc+Ntitle)<br>\n",
    "for item in document_vocab do <br>\n",
    "    likelihood[item,c] = (count(witem|c)+count(wtitle|c)+l)/(Sum(count(w|c))+|V|.l+|x|.l)\n",
    "return Prior[c],likelihood[item,c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From documents to feature vectors\n",
    "This section illustratess the prototypical components of machine learning pipeline for an NLP task, in this case document classification:\n",
    "\n",
    "1. Read document examples (train, devel, test) from files with a predefined format:\n",
    "    - assume one document per line, usign the format \"\\<label\\> \\<text\\>\".\n",
    "\n",
    "2. Tokenize each document:\n",
    "    - using a spaCy tokenizer.\n",
    "\n",
    "3. Feature extractors:\n",
    "    - so far, just words.\n",
    "\n",
    "4. Process each document into a feature vector:\n",
    "    - map document to a dictionary of feature names.\n",
    "    - map feature names to unique feature IDs.\n",
    "    - each document is a feature vector, where each feature ID is mapped to a feature value (e.g. word occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy tokenizer.\n",
    "spacy_nlp = English()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats, new_feats):\n",
    "    for feat in new_feats:\n",
    "        if feat in feats:\n",
    "            feats[feat] += new_feats[feat]\n",
    "        else:\n",
    "            feats[feat] = new_feats[feat]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the document, runs all the feature extractors on it and assembles the extracted features into a dictionary mapping feature names to feature values. It is important that feature names do not conflict with each other, i.e. different features should have different names. Each document will have its own dictionary of features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                #print(new_examples[:100])\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Naive Bayes classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    \n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28692\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datapath = '../data'\n",
    "\n",
    "train_file = os.path.join(datapath, 'imdb_sentiment_train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "\n",
    "dev_file = os.path.join(datapath, 'imdb_sentiment_dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when using only alpha tokens. This can be done by changing the tokenizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 25054\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.785\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer1(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Keep in the tokens list only those whose text is made up solely from letters.\n",
    "    tokenset = []\n",
    "    for token in tokens:\n",
    "        if(token.is_alpha):\n",
    "            tokenset.append(token.text)\n",
    "    \n",
    "    return tokenset\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but lowercase all tokens before using as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21708\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.784\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer2(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Keep in the tokens list only those whose text is made up solely from letters.\n",
    "    # Return a list of lowercased token text.\n",
    "    tokenset = [] \n",
    "    for token in tokens:\n",
    "        if(token.is_alpha):\n",
    "            tokenset.append(token.text.lower())\n",
    "    \n",
    "    return tokenset\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but lowercase only tokens that appear at the beginning of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28692\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = English()\n",
    "#sentencizer = spacy.pipeline.Sentencizer()\n",
    "spacy_nlp.add_pipe('sentencizer')  \n",
    "\n",
    "\n",
    "def spacy_tokenizer3(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    token = []\n",
    "    # YOUR CODE HERE\n",
    "    for sent in doc.sents:\n",
    "        for tok in sent:\n",
    "            if(tok.is_sent_start and tok.is_alpha):\n",
    "                token.append(tok.text.lower())\n",
    "            else:\n",
    "                token.append(tok.text)\n",
    "    \n",
    "    return token\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spacy_tokenizer2 (only alpha tokens, lowered all) and display the top 10 most frequent tokens in the vocabulary, as a list of tuples (token, frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21708\n",
      "('WORD_the', 20105)\n",
      "('WORD_and', 9876)\n",
      "('WORD_a', 9749)\n",
      "('WORD_of', 9029)\n",
      "('WORD_to', 8182)\n",
      "('WORD_is', 6605)\n",
      "('WORD_in', 5598)\n",
      "('WORD_it', 5387)\n",
      "('WORD_i', 4773)\n",
      "('WORD_this', 4448)\n",
      "('WORD_that', 4252)\n",
      "('WORD_was', 2961)\n",
      "('WORD_with', 2693)\n",
      "('WORD_as', 2682)\n",
      "('WORD_movie', 2574)\n",
      "('WORD_for', 2556)\n",
      "('WORD_but', 2435)\n",
      "('WORD_film', 2359)\n",
      "('WORD_you', 2066)\n",
      "('WORD_on', 2061)\n"
     ]
    }
   ],
   "source": [
    "# First, count token occurrences across all examples, where features are still strings. \n",
    "def create_feature_counts(examples):\n",
    "    feature_counts = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for items in examples:\n",
    "        add_features(feature_counts,items)\n",
    "                \n",
    "    return feature_counts\n",
    "\n",
    "# Create features for all training examples, compute feature counts \n",
    "def fcounts_from_train(trainX, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_counts))\n",
    "\n",
    "    return feature_counts\n",
    "\n",
    "# Return a list of the top K most frequent tokens in the vocabulary.\n",
    "def topK_tokens(vocab, k):\n",
    "    # YOUR CODE HERE\n",
    "    ktopTokens = []\n",
    "    sorted_vocab = sorted(vocab.items(), key = lambda item:item[1], reverse = True)\n",
    "    for i in range(k):\n",
    "        ktopTokens.append(sorted_vocab[i])\n",
    "    \n",
    "    return ktopTokens\n",
    "\n",
    "vocab = fcounts_from_train(trainX, features, spacy_tokenizer2)\n",
    "stop_words = topK_tokens(vocab, 20)\n",
    "for item in stop_words:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when ignoring the top 20 stop words. Use spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1n/3021sx6n2gn99kbcsy7kgsfm0000gn/T/ipykernel_24198/2615408781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Evaluate NB model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_tokenizer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation pipeline for the Naive Bayes classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    \n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    stop_words = topK_tokens(vocab, 20)\n",
    "\n",
    "    # Remove from each example features that appear in the stop words list.\n",
    "    # YOUR CODE HERE.\n",
    "    for item in trainX_feat:\n",
    "        for key in item.items():\n",
    "            for word in stop_words:\n",
    "                if(key[0] == word[0]):\n",
    "                    item.pop(key[0])\n",
    "    \n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "    \n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when ignoring words that appear less than 5 times. Use spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 5573\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.791\n"
     ]
    }
   ],
   "source": [
    "def wordFreqLT5(vocab):\n",
    "    listOfWords = []\n",
    "    for a,b in vocab.items():\n",
    "        if(b<5):\n",
    "            listOfWords.append(a)\n",
    "    return listOfWords\n",
    "\n",
    "def trainAndTest(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    \n",
    "    wordOccLT5 = wordFreqLT5(vocab)\n",
    "    \n",
    "    for item in trainX_feat:\n",
    "        for key in item.copy().items():\n",
    "            for word in wordOccLT5:\n",
    "                if(key[0] == word):\n",
    "                    item.pop(key[0])\n",
    "    \n",
    "    \n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "    \n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "\n",
    "trainAndTest(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Multinomial Bayes\n",
    "*Mandatory for graduate students, optional for undergraduate students*\n",
    "\n",
    "Write code for transforming documents to features such that features are Boolean and only represent whether a word occurred in a document, as in the Binary Multinomial Naive Bayes discussed in class. Evaluate the Naive Bayes model with this feature representation, using spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21708\n",
      "Accuracy: 0.807\n"
     ]
    }
   ],
   "source": [
    "def preProcessing(training_ds_words,feature_funcs,tokenizer):\n",
    "    examples = []\n",
    "    \n",
    "    count = 0\n",
    "    for item in training_ds_words:\n",
    "        wordCnt = {}\n",
    "        tokenset = tokenizer(item)\n",
    "        x = word_features(tokenset)\n",
    "        x_noDups = removeDuplicates(x)\n",
    "        add_features(wordCnt,x_noDups)\n",
    "        examples.append(wordCnt)\n",
    "    return examples\n",
    "\n",
    "def removeDuplicates(tokenDictionary):\n",
    "    for key,value in tokenDictionary.items():\n",
    "        if(value > 1):\n",
    "            tokenDictionary[key] = 1\n",
    "    return tokenDictionary\n",
    "\n",
    "trainX_feat = preProcessing(trainX,features,spacy_tokenizer2)\n",
    "\n",
    "feature_vocab = create_vocab(trainX_feat)\n",
    "print('Vocabulary size: %d' % len(feature_vocab))\n",
    "    \n",
    "trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "nb_model = MultinomialNB(alpha = 1.0)\n",
    "nb_model.fit(trainX_ids, trainY)\n",
    "features = [word_features]   \n",
    "devX_feat = preProcessing(devX,features, spacy_tokenizer2)\n",
    "devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "Anything extra goes here. For example, implement NB from scratch in a separate module nbayes.py and use it for the exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "Include an analysis of the results that you obtained in the experiments above. Take advantage of the Jupyter Notebook markdown language, which can also process Latex and HTML, to format your report so that it looks professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis has been presented in a seperate latex file attached along with the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
