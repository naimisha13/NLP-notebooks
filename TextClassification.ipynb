{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "1. Propose a custom text classification task and create a corpus of documents that are labeled for this task.\n",
    "\n",
    "2. Propose discriminative features to be included in the feature vector representation for a document and evaluate their utility by training and testing Logistic Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your Name Here: Shreyas Lokesha\n",
    "##### UNC-Id: 801210964\n",
    "##### e-mail: slokesha@uncc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus acquisition and formatting\n",
    "\n",
    "1. Find at least 300 documents for some topic that interests you, along with a single label for each document.\n",
    "\n",
    "\t- The more data in your collection, the better your classification models will tend to perform on it.\n",
    "    \n",
    "\n",
    "2. Partition your data into three files train.txt, dev.txt and test.txt, with training containing 80% of the documents, development 10% and test 10%.\n",
    "\n",
    "\n",
    "3. All of the data must be in a common format:\n",
    "\n",
    "    - One example per line, using the format \"\\<label\\> \\<text\\>\", i.e. the line starts with the label, followed by a white space, followed by the text of the document.\n",
    "    - Replace all newlines in the text with \\<NEWLINE\\> and tab characters with \\<TAB\\>.\n",
    "    - See corpus under *data/sentiment/* for examples.\n",
    "    \n",
    "    \n",
    "4. Your choice of documents and labels is completely up to you. Possible sources of data:\n",
    "\n",
    "    - **Project Gutenberg**: Metadata is available at this <a href=\"https://github.com/hugovk/gutenberg-metadata\">Github repo</a> along with URLs for the texts. Labels here can be author, subject, genre, etc.\n",
    "\t- **News articles**: Crawl news articles from different domains (e.g,. CNN, FoxNews); the label for each article is the domain.\n",
    "\t- **Movie summaries**: Labels here can be any categorical metadata aspect (genre, release date); note real-valued metadata (like box office, runtime) can be discretized by selecting some reasonable thresholds.\n",
    "\t- **Tweets**: Download your own tweets. Labels here can be any categorical metadata included in the tweet, or labels you add by hand (e.g., sarcasm).\n",
    "    \n",
    "    \n",
    "5. Additional requirements:\n",
    "\n",
    "    - No sentiment classification.\n",
    "    - **Undergraduate students**: It is acceptable to use an existing dataset, e.g. from kaggle.com or other repositories. However, it is preferable that you create your own dataset.\n",
    "    - **Graduate students**: It is important that you create your own dataset. This can also serve as the basis for your project, if you choose to work on text classification for it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task and Dataset description\n",
    "\n",
    "Describe your data. What is the source of the documents, and what do the labels mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// YOUR CONTRIBUTION HERE <br>\n",
    "The dataset compiled consists of iconic movie dialogues from Marvel and DC movies along with the labels.<br>\n",
    "The labels used in the dataset are Marvel and DC and we have around 320 movie dialogues each from famous Marvel and DC movies.<br>\n",
    "The name of the movie is also present as a text in the dataset marked within '\\[' and '\\]' symbols.<br>\n",
    "Using this dataset, we perform text classification. <br>\n",
    "The goal of this text classification excercise is to predict whether the movie dialogues are from Marvel or DC movies.<br>\n",
    "Sources of data: rotten tomatoes website\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset reading and statistics\n",
    "\n",
    "Change the path to match the directory containing your data and execute the *read_examples()* function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: {'Marvel': 128, 'DC': 128}\n",
      "Development examples: {'Marvel': 16, 'DC': 16}\n",
      "Test examples: {'Marvel': 16, 'DC': 16}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r',encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "            \n",
    "    return X, Y\n",
    "\n",
    "def label_counts(Y):\n",
    "    labels = {}\n",
    "    for l in Y:\n",
    "        if l in labels:\n",
    "            labels[l] += 1\n",
    "        else:\n",
    "            labels[l] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "datapath = \"../data/MarvelDCClassifier/\"\n",
    "\n",
    "train_file = os.path.join(datapath, 'train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "print(\"Training examples:\", label_counts(trainY))\n",
    "\n",
    "dev_file = os.path.join(datapath, 'dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "print(\"Development examples:\", label_counts(devY))\n",
    "\n",
    "\n",
    "test_file = os.path.join(datapath, 'test.txt')\n",
    "testX, testY = read_examples(test_file)\n",
    "print(\"Test examples:\", label_counts(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From documents to feature vectors\n",
    "This section illustratess the prototypical components of machine learning pipeline for an NLP task, in this case document classification:\n",
    "\n",
    "1. Read document examples (train, devel, test) from files with a predefined format:\n",
    "    - assume one document per line, using the format \"\\<label\\> \\<text\\>\".\n",
    "\n",
    "2. Tokenize each document:\n",
    "    - using a spaCy tokenizer.\n",
    "\n",
    "3. Feature extractors:\n",
    "    - so far, just words.\n",
    "\n",
    "4. Process each document into a feature vector:\n",
    "    - map document to a dictionary of feature names.\n",
    "    - map feature names to unique feature IDs.\n",
    "    - each document is a feature vector, where each feature ID is mapped to a feature value (e.g. word occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy tokenizer.\n",
    "spacy_nlp = English()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats, new_feats):\n",
    "    for feat in new_feats:\n",
    "        if feat in feats:\n",
    "            feats[feat] += new_feats[feat]\n",
    "        else:\n",
    "            feats[feat] = new_feats[feat]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the document, runs all the feature extractors on it and assembles the extracted features into a dictionary mapping feature names to feature values. It is important that feature names do not conflict with each other, i.e. different features should have different names. Each document will have its own dictionary of features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Logistic Regression classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train LR model.\n",
    "    lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "    lr_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test LR model.\n",
    "    print('Accuracy: %.3f' % lr_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Create two new feature functions, and include them in the *features* list.\n",
    "\n",
    "- A passing grade will be given to generic features that apply across arbitrary text classification problems (e.g., a feature for bigrams);\n",
    "\n",
    "- A better grade will be given for features that reveal your own understanding of your data. What features do you think will help for your particular problem? Would features based on higher level NLP processing tasks (anmed entity recognition, syntactic parsing, coreference resolution) be useful? Your grade is not tied to whether accuracy goes up or down, so be creative!\n",
    "\n",
    "- You are free to read in any other external resources you like (dictionaries, document metadata, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCMarvel_Lexicon_extract(tokens):\n",
    "    DC_Lexicon_set = set(('alfred','amazons','aquaman','arthur','arthur aurry','atlan','atlanna','atlantis','atlanteans','bat','batman','barry','barry allen','bruce wayne','clark','clark kent','codex','diana','deadshot','floyd lawton',\n",
    "                         'darksiede','doomsday','dent','enigma','enchantress','faora','gotham','harley quinn','harvey','harvey dent','joker','jor-el','kal','kal-el','krypton','kryptonite','kryptonian','lex','luthor','lois','lois lane','martha','ocean master','orm','parademons','steppenwolf','superman',\n",
    "                         'trevor','two-face','wayne','zod'))\n",
    "    Marvel_Lexicon_set = set(('agent','asgard','asgardian','askervarian','avenger','avengers','bifrost','bucky','cap','captain','chitauri','coulson','dormammu','eytri','fandral','frigga','gamora','groot','hank','hulk','hydra','jarvis','jane','jotunheim','knowhere','loki',\n",
    "                             'odin','panther','parker','pepper','peter','phil','pym','quill','rocket','shield','sif','steve','stark','strange','spider','tesseract','thanos','thor'))\n",
    "    lex_feat = {}\n",
    "    for item in tokens:\n",
    "        tempStore = item.lower()\n",
    "        if(tempStore in DC_Lexicon_set):\n",
    "            entry = \"DCLex_\"+str(item)\n",
    "            if(entry not in lex_feat):\n",
    "                lex_feat[entry] = 1\n",
    "            else:\n",
    "                lex_feat[entry] += 1\n",
    "        if(tempStore in Marvel_Lexicon_set):\n",
    "            entry = \"MarvelLex_\"+str(item)\n",
    "            if(entry not in lex_feat):\n",
    "                lex_feat[entry] = 1\n",
    "            else:\n",
    "                lex_feat[entry] += 1\n",
    "                \n",
    "    return lex_feat\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCMarvel_MovieName_Lex(tokens):\n",
    "    DC_Movies = set(('aquaman','batman begins','batman v superman: dawn of justice','justice league','man of steel','suicide squad','the dark knight','the dark knight rises','wonder woman'))\n",
    "    Marvel_Movies = set(('ant-man','ant-man and the wasp','avengers: age of ultron','avengers:infinity war','avengers:endgame','captain america:the first avenger',\n",
    "                         'captain america:the winter soldier','captain america: civil war','doctor strange','guardians of the galaxy','iron man','iron man 2','the avengers','the incredible hulk','thor','thor:the dark world'\n",
    "                         ,'thor:ragnarok'))\n",
    "    \n",
    "    feat_movies = {'DC_Mov':0,'Marvel_Mov':0}\n",
    "    movie_name = ''\n",
    "    index = 0\n",
    "    dc_mov_count = 0\n",
    "    marvel_mov_count = 0\n",
    "    stg = ''\n",
    "    for item in tokens:\n",
    "        if('[' in item):\n",
    "            index = tokens.index(item)\n",
    "            subIndex = item.index('[')\n",
    "            stg = stg + item[subIndex+1:]\n",
    "            if(tokens[index] != ']'):\n",
    "                index = index + 1\n",
    "                movie_name = movie_name + stg + \" \" + tokens[index]+\" \"\n",
    "                \n",
    "    dc_match = False\n",
    "    marvel_match = False\n",
    "    preprocessStg = movie_name.lower().rstrip()\n",
    "    for movie in DC_Movies:\n",
    "        if(movie == preprocessStg):\n",
    "            dc_match = True\n",
    "            feat_movies['DC_Mov'] += 1\n",
    "    \n",
    "    for movie in Marvel_Movies:\n",
    "        if(movie == preprocessStg):\n",
    "            marvel_match = True\n",
    "            feat_movies['Marvel_Mov'] += 1\n",
    "    \n",
    "    \n",
    "               \n",
    "    return feat_movies\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of your features by training and testing with and without each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Vocabulary size: 1500\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datapath = '../data/MarvelDCClassifier'\n",
    "\n",
    "train_file = os.path.join(datapath, 'Train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "\n",
    "dev_file = os.path.join(datapath, 'Dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use. Do this multiple times, with and without the new features\n",
    "features = [word_features,DCMarvel_MovieName_Lex]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "\n",
    "*Mandatory for graduate students, optional for undergraduate students.*\n",
    "\n",
    "Take your best classifier and plot a <a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">Receiver Operating Characteristic (ROC)</a> curve by varying a threshold on the probabilistic output. You can use the <a href=\"https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\">sklearn implementation</a>, or implement your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Accuracy: 0.875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "train_file = os.path.join(datapath, 'Train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "dev_file = os.path.join(datapath, 'Dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "feature_functions = [word_features,DCMarvel_Lexicon_extract]\n",
    "trainX_feat = docs2features(trainX, feature_functions, spacy_tokenizer)\n",
    "feature_vocab = create_vocab(trainX_feat)\n",
    "trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "lr_model.fit(trainX_ids, trainY)\n",
    "devX_feat = docs2features(devX, feature_functions, spacy_tokenizer)\n",
    "devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "print('Accuracy: %.3f' % lr_model.score(devX_ids, devY))\n",
    "lr_probability = lr_model.predict_proba(devX_ids)[:,1]\n",
    "devY_class = []\n",
    "for item in devY:\n",
    "    if(item == 'DC'):\n",
    "        devY_class.append(0)\n",
    "    else:\n",
    "        devY_class.append(1)\n",
    "        \n",
    "fpr,tpr,threshold = roc_curve(devY_class,lr_probability)\n",
    "\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.plot(fpr,tpr,color=\"blue\")\n",
    "plt.xlabel('False Positive')\n",
    "plt.ylabel('True Positive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "\n",
    "Anything extra goes here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Vocabulary size: 1500\n",
      "Accuracy: 0.938\n"
     ]
    }
   ],
   "source": [
    "def DCMarvel_Lex_counter(tokens):\n",
    "    DC_Lexicon_set = set(('alfred','amazons','aquaman','arthur','arthur aurry','atlan','atlanna','atlantis','atlanteans','bat','batman','barry','barry allen','bruce wayne','clark','clark kent','codex','diana','deadshot','floyd lawton',\n",
    "                         'darksiede','doomsday','dent','enigma','enchantress','faora','gotham','harley quinn','harvey','harvey dent','joker','jor-el','kal','kal-el','krypton','kryptonite','kryptonian','lex','luthor','lois','lois lane','martha','ocean master','orm','parademons','steppenwolf','superman',\n",
    "                         'trevor','two-face','wayne','zod'))\n",
    "    Marvel_Lexicon_set = set(('agent','asgard','asgardian','askervarian','avenger','avengers','bifrost','bucky','cap','captain','chitauri','coulson','dormammu','eytri','fandral','frigga','gamora','groot','hank','hulk','hydra','jarvis','jane','jotunheim','knowhere','loki',\n",
    "                             'odin','panther','parker','pepper','peter','phil','pym','quill','rocket','shield','sif','steve','stark','strange','spider','tesseract','thanos','thor'))\n",
    "    \n",
    "    feat_DCMCounter = {\"DC_Lex_count\":0,\"Marvel_Lex_count\": 0}\n",
    "   \n",
    "    for item in tokens:\n",
    "        if(item.lower() in DC_Lexicon_set):\n",
    "            feat_DCMCounter[\"DC_Lex_count\"] += 1\n",
    "        elif(item.lower() in Marvel_Lexicon_set):\n",
    "            feat_DCMCounter[\"Marvel_Lex_count\"] += 1\n",
    "    \n",
    "    return feat_DCMCounter\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "datapath = '../data/MarvelDCClassifier'\n",
    "train_file = os.path.join(datapath, 'Train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "\n",
    "dev_file = os.path.join(datapath, 'Dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use. Do this multiple times, with and without the new features\n",
    "features = [word_features,DCMarvel_Lex_counter]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "Include an analysis of the results that you obtained in the experiments above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
