{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with RNNs (LSTMs) \n",
    "\n",
    "In this assignment you will experiment with training and evaluating sentiment classification models that use recurrent neural networks (RNNs) implemented in PyTorch. For this, you will need to install the <a href=\"https://pytorch.org/\">PyTorch</a> package, using the instructions below (installation with <a href=\"https://www.anaconda.com/\">conda</a> is recomended):\n",
    "\n",
    "https://pytorch.org/get-started/locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Your Name Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *lstm-sentiment.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *lstm-sentiment.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas. Make sure the PDF and notebook show the outputs of the training and evaluation procedures. Also upload the **output** on the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from sentiment_data import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import NamedTuple\n",
    "\n",
    "class HyperParams(NamedTuple):\n",
    "    lstm_size: int\n",
    "    hidden_size: int\n",
    "    lstm_layers: int\n",
    "    drop_out: float\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    seq_max_len: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based training and evaluation procedures\n",
    "\n",
    "We will use the RNNet class defined in `models.py` that uses LSTMs implemented in PyTorch. Depending on the options, this class runs one LSTM (forward) or two LSTMS (bidirectional, forward-backward) on the padded input text. The last state (or concatenated last states), or the average of the states, is used as input to a fully connected network with 3 hidden layers, with a final output sigmoid node computing the probability of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure for LSTM-based models\n",
    "def train_model(hp: HyperParams,\n",
    "                train_exs: List[SentimentExample],\n",
    "                dev_exs: List[SentimentExample],\n",
    "                test_exs: List[SentimentExample], \n",
    "                word_vectors: WordEmbeddings,\n",
    "                use_average, bidirectional):\n",
    "    train_size = len(train_exs)\n",
    "    class_num = 1\n",
    "    \n",
    "    # Specify training on gpu: set to False to train on cpu\n",
    "    use_gpu = True # torch.cuda.is_available()\n",
    "    if use_gpu: # Set tensor type when using GPU\n",
    "        float_type = torch.cuda.FloatTensor\n",
    "    else: # Set tensor type when using CPU\n",
    "        float_type = torch.FloatTensor\n",
    "        \n",
    "    # To get you started off, we'll pad the training input to 60 words to make it a square matrix.\n",
    "    train_mat = np.asarray([pad_to_length(np.array(ex.indexed_words), hp.seq_max_len) for ex in train_exs])\n",
    "    # Also store the actual sequence lengths.\n",
    "    train_seq_lens = np.array([len(ex.indexed_words) for ex in train_exs])\n",
    "    \n",
    "    # Training input reversed, useful is using bidirectional LSTM.\n",
    "    train_mat_rev = np.asarray([pad_to_length(np.array(ex.get_indexed_words_reversed()), hp.seq_max_len) for ex in train_exs])\n",
    "\n",
    "    # Extract labels.\n",
    "    train_labels_arr = np.array([ex.label for ex in train_exs])\n",
    "    targets = train_labels_arr\n",
    "    \n",
    "    # Extract embedding vectors.\n",
    "    embed_size = word_vectors.get_embedding_length()\n",
    "    embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "    \n",
    "    # Create RNN model.\n",
    "    rnnModel = RNNet(hp.lstm_size, hp.hidden_size, hp.lstm_layers, hp.drop_out,\n",
    "                     class_num, word_vectors, \n",
    "                     use_average, bidirectional,\n",
    "                     use_gpu =use_gpu)\n",
    "    \n",
    "    # If GPU is available, then run experiments on GPU\n",
    "    if use_gpu:\n",
    "        rnnModel.cuda()\n",
    "    \n",
    "    # Specify optimizer.\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, rnnModel.parameters()), \n",
    "                           lr = 5e-3, weight_decay  =5e-3, betas = (0.9, 0.9))\n",
    "    \n",
    "    # Define loss function: Binary Cross Entropy loss for logistic regression (binary classification).\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    \n",
    "    # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "    x = np.zeros((train_size, hp.seq_max_len, embed_size))\n",
    "    x_rev = np.zeros((train_size, hp.seq_max_len, embed_size))\n",
    "    for i in range(train_size):\n",
    "        x[i] = embeddings_vec[train_mat[i].astype(int)]\n",
    "        x_rev[i] = embeddings_vec[train_mat_rev[i].astype(int)]\n",
    "    \n",
    "    # Train the RNN model, gradient descent loop over minibatches.\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        rnnModel.train()\n",
    "        \n",
    "        ex_idxs = [i for i in range(train_size)]\n",
    "        random.shuffle(ex_idxs)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        start = 0\n",
    "        while start < train_size:\n",
    "            end = min(start + hp.batch_size, train_size)\n",
    "            \n",
    "            # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "            x_batch = form_input(x[ex_idxs[start:end]]).type(float_type)\n",
    "            x_batch_rev = form_input(x_rev[ex_idxs[start:end]]).type(float_type)\n",
    "            y_batch = form_input(targets[ex_idxs[start:end]]).type(float_type)\n",
    "            seq_lens_batch = train_seq_lens[ex_idxs[start:end]]\n",
    "            \n",
    "            # Compute output probabilities over all examples in minibatch.\n",
    "            probs = rnnModel(x_batch, x_batch_rev, seq_lens_batch).flatten()\n",
    "            \n",
    "            # Compute loss over all examples in minibatch.\n",
    "            loss = criterion(probs, y_batch)\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            start = end\n",
    "            \n",
    "        print(\"Loss on epoch %i: %f\" % (epoch, total_loss))\n",
    "        \n",
    "        # Print accuracy on training and development data.\n",
    "        if epoch % 10 == 0:\n",
    "            acc = eval_model(rnnModel, train_exs, embeddings_vec, hp.seq_max_len)\n",
    "            print('Epoch', epoch, ': Accuracy on training set:', acc)\n",
    "            acc = eval_model(rnnModel, dev_exs, embeddings_vec, hp.seq_max_len)\n",
    "            print('Epoch', epoch, ': Accuracy on development set:', acc)\n",
    "            \n",
    "    # Evaluate model on the training dataset.\n",
    "    acc = eval_model(rnnModel, train_exs, embeddings_vec, hp.seq_max_len)\n",
    "    print('Accuracy on training set:', acc)\n",
    "    \n",
    "    # Evaluate model on the development dataset.\n",
    "    acc = eval_model(rnnModel, dev_exs, embeddings_vec, hp.seq_max_len)\n",
    "    print('Accuracy on develpment set:', acc)\n",
    "    \n",
    "    return rnnModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the testing (evaluation) procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test examples and return predicted labels or accuracy.\n",
    "def eval_model(model, exs, embeddings_vec, seq_max_len, pred_only = False):\n",
    "    # Put model in evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract size pf word embedding.\n",
    "    embed_size = len(embeddings_vec[0])\n",
    "    \n",
    "    # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "    exs_mat = np.asarray([pad_to_length(np.array(ex.indexed_words), seq_max_len) for ex in exs])\n",
    "    exs_mat_rev = np.asarray([pad_to_length(np.array(ex.get_indexed_words_reversed()), seq_max_len) for ex in exs])\n",
    "    exs_seq_lens = np.array([len(ex.indexed_words) for ex in exs])\n",
    "    \n",
    "    # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "    x = np.zeros((len(exs), seq_max_len, embed_size))\n",
    "    x_rev = np.zeros((len(exs), seq_max_len, embed_size))\n",
    "    for i,ex in enumerate(exs):\n",
    "        x[i] = embeddings_vec[exs_mat[i].astype(int)]\n",
    "        x_rev[i] = embeddings_vec[exs_mat_rev[i].astype(int)]\n",
    "        \n",
    "    x = form_input(x)\n",
    "    x_rev = form_input(x_rev)\n",
    "    \n",
    "    # Run the model on the test examples.\n",
    "    preds = model(x, x_rev, exs_seq_lens).cpu().detach().numpy().flatten()\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    \n",
    "    if pred_only == True:\n",
    "        return preds\n",
    "    else:\n",
    "        targets = np.array([ex.label for ex in exs])\n",
    "        return np.mean(preds == targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental evaluations on the Rotten Tomatoes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, code for reading the examples and the corresponding GloVe word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 30135 vectors of size 300\n",
      "8530 / 1066 / 1066 train / dev / test examples\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "word_vecs_path = '../data/glove.6B.300d-relativized.txt'\n",
    "\n",
    "train_path = '../data/rt/train.txt'\n",
    "dev_path = '../data/rt/dev.txt'\n",
    "blind_test_path = '../data/rt/test-blind.txt'\n",
    "test_output_path = 'test-blind.output.txt'\n",
    "\n",
    "word_vectors = read_word_embeddings(word_vecs_path)\n",
    "word_indexer = word_vectors.word_indexer\n",
    "\n",
    "train_exs = read_and_index_sentiment_examples(train_path, word_indexer)\n",
    "dev_exs = read_and_index_sentiment_examples(dev_path, word_indexer)\n",
    "test_exs = read_and_index_sentiment_examples(blind_test_path, word_indexer)\n",
    "\n",
    "print(repr(len(train_exs)) + \" / \" + \n",
    "      repr(len(dev_exs)) + \" / \" + \n",
    "      repr(len(test_exs)) + \" train / dev / test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use only the last state from one LSTM\n",
    "\n",
    "Evaluate One LSTM + fully connected network, use last hidden state of LSTM. If the evaluation takes more than 1 hour on your computer, try reducing `lst_size`, `hidden_size`, `batch_size` and even `num_epochs`.\n",
    "\n",
    "Our accuracy on development data is 75.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.267474\n",
      "Epoch 0 : Accuracy on training set: 0.5847596717467761\n",
      "Epoch 0 : Accuracy on development set: 0.5956848030018762\n",
      "Loss on epoch 1: 5.798876\n",
      "Loss on epoch 2: 5.336525\n",
      "Loss on epoch 3: 4.944349\n",
      "Loss on epoch 4: 4.594477\n",
      "Loss on epoch 5: 4.402339\n",
      "Loss on epoch 6: 4.373571\n",
      "Loss on epoch 7: 4.300267\n",
      "Loss on epoch 8: 4.347034\n",
      "Loss on epoch 9: 4.273001\n",
      "Loss on epoch 10: 4.303786\n",
      "Epoch 10 : Accuracy on training set: 0.793200468933177\n",
      "Epoch 10 : Accuracy on development set: 0.7560975609756098\n",
      "Loss on epoch 11: 4.128220\n",
      "Loss on epoch 12: 4.127317\n",
      "Loss on epoch 13: 4.068804\n",
      "Loss on epoch 14: 3.973914\n",
      "Loss on epoch 15: 4.045157\n",
      "Loss on epoch 16: 3.977415\n",
      "Loss on epoch 17: 4.071261\n",
      "Loss on epoch 18: 3.851947\n",
      "Loss on epoch 19: 3.817534\n",
      "Loss on epoch 20: 3.910577\n",
      "Epoch 20 : Accuracy on training set: 0.8055099648300117\n",
      "Epoch 20 : Accuracy on development set: 0.7495309568480301\n",
      "Loss on epoch 21: 3.820059\n",
      "Loss on epoch 22: 3.769975\n",
      "Loss on epoch 23: 3.690644\n",
      "Loss on epoch 24: 3.656665\n",
      "Loss on epoch 25: 3.635234\n",
      "Loss on epoch 26: 3.734457\n",
      "Loss on epoch 27: 3.585371\n",
      "Loss on epoch 28: 3.457077\n",
      "Loss on epoch 29: 3.603906\n",
      "Loss on epoch 30: 3.486769\n",
      "Epoch 30 : Accuracy on training set: 0.8311840562719812\n",
      "Epoch 30 : Accuracy on development set: 0.7692307692307693\n",
      "Loss on epoch 31: 3.562800\n",
      "Loss on epoch 32: 3.468657\n",
      "Loss on epoch 33: 3.342267\n",
      "Loss on epoch 34: 3.356163\n",
      "Loss on epoch 35: 3.360148\n",
      "Loss on epoch 36: 3.244395\n",
      "Loss on epoch 37: 3.259465\n",
      "Loss on epoch 38: 3.390141\n",
      "Loss on epoch 39: 3.329961\n",
      "Loss on epoch 40: 3.192054\n",
      "Epoch 40 : Accuracy on training set: 0.8413833528722157\n",
      "Epoch 40 : Accuracy on development set: 0.7626641651031895\n",
      "Loss on epoch 41: 3.080724\n",
      "Loss on epoch 42: 2.993246\n",
      "Loss on epoch 43: 3.260773\n",
      "Loss on epoch 44: 3.343589\n",
      "Loss on epoch 45: 3.364152\n",
      "Loss on epoch 46: 3.113914\n",
      "Loss on epoch 47: 3.052620\n",
      "Loss on epoch 48: 2.958403\n",
      "Loss on epoch 49: 2.901864\n",
      "Accuracy on training set: 0.8493552168815943\n",
      "Accuracy on develpment set: 0.7523452157598499\n",
      "Prediction written to file for Rotten Tomatoes dataset.\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = False\n",
    "bidirectional = False\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the average of all states from one LSTM\n",
    "\n",
    "Evaluate One LSTM + fully connected network, use average of all states of the lstm.\n",
    "\n",
    "Our accuracy on development data is 77.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.157734\n",
      "Epoch 0 : Accuracy on training set: 0.6492379835873388\n",
      "Epoch 0 : Accuracy on development set: 0.6622889305816135\n",
      "Loss on epoch 1: 5.381994\n",
      "Loss on epoch 2: 4.778000\n",
      "Loss on epoch 3: 4.582939\n",
      "Loss on epoch 4: 4.454709\n",
      "Loss on epoch 5: 4.404754\n",
      "Loss on epoch 6: 4.420320\n",
      "Loss on epoch 7: 4.258039\n",
      "Loss on epoch 8: 4.284607\n",
      "Loss on epoch 9: 4.263848\n",
      "Loss on epoch 10: 4.221387\n",
      "Epoch 10 : Accuracy on training set: 0.7749120750293084\n",
      "Epoch 10 : Accuracy on development set: 0.7439024390243902\n",
      "Loss on epoch 11: 4.174233\n",
      "Loss on epoch 12: 4.122141\n",
      "Loss on epoch 13: 4.154859\n",
      "Loss on epoch 14: 4.099349\n",
      "Loss on epoch 15: 4.126765\n",
      "Loss on epoch 16: 4.133695\n",
      "Loss on epoch 17: 4.122970\n",
      "Loss on epoch 18: 4.016438\n",
      "Loss on epoch 19: 4.001831\n",
      "Loss on epoch 20: 3.994933\n",
      "Epoch 20 : Accuracy on training set: 0.7994138335287222\n",
      "Epoch 20 : Accuracy on development set: 0.7607879924953096\n",
      "Loss on epoch 21: 3.870770\n",
      "Loss on epoch 22: 3.868393\n",
      "Loss on epoch 23: 3.862161\n",
      "Loss on epoch 24: 3.803983\n",
      "Loss on epoch 25: 3.824103\n",
      "Loss on epoch 26: 3.726716\n",
      "Loss on epoch 27: 3.670806\n",
      "Loss on epoch 28: 3.672844\n",
      "Loss on epoch 29: 3.633744\n",
      "Loss on epoch 30: 3.569256\n",
      "Epoch 30 : Accuracy on training set: 0.8246189917936694\n",
      "Epoch 30 : Accuracy on development set: 0.7570356472795498\n",
      "Loss on epoch 31: 3.584367\n",
      "Loss on epoch 32: 3.492942\n",
      "Loss on epoch 33: 3.553848\n",
      "Loss on epoch 34: 3.490386\n",
      "Loss on epoch 35: 3.470421\n",
      "Loss on epoch 36: 3.407650\n",
      "Loss on epoch 37: 3.358163\n",
      "Loss on epoch 38: 3.380332\n",
      "Loss on epoch 39: 3.241984\n",
      "Loss on epoch 40: 3.310562\n",
      "Epoch 40 : Accuracy on training set: 0.8279015240328254\n",
      "Epoch 40 : Accuracy on development set: 0.7373358348968105\n",
      "Loss on epoch 41: 3.241687\n",
      "Loss on epoch 42: 3.178872\n",
      "Loss on epoch 43: 3.224559\n",
      "Loss on epoch 44: 3.157825\n",
      "Loss on epoch 45: 3.230515\n",
      "Loss on epoch 46: 3.115021\n",
      "Loss on epoch 47: 3.373563\n",
      "Loss on epoch 48: 3.253216\n",
      "Loss on epoch 49: 3.061435\n",
      "Accuracy on training set: 0.8603751465416178\n",
      "Accuracy on develpment set: 0.7645403377110694\n",
      "Prediction written to file for Rotten Tomatoes dataset.\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = True\n",
    "bidirectional = False\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a bidirectional LSTM, concatenate last states\n",
    "\n",
    "Evaluate Two LSTMs (bidirectional) + fully connected network, concatenate their last states.\n",
    "\n",
    "Our accuracy on development data is 76.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.474929\n",
      "Epoch 0 : Accuracy on training set: 0.5371629542790153\n",
      "Epoch 0 : Accuracy on development set: 0.5403377110694184\n",
      "Loss on epoch 1: 6.134594\n",
      "Loss on epoch 2: 5.732855\n",
      "Loss on epoch 3: 4.986666\n",
      "Loss on epoch 4: 4.877936\n",
      "Loss on epoch 5: 4.496234\n",
      "Loss on epoch 6: 4.401465\n",
      "Loss on epoch 7: 4.166438\n",
      "Loss on epoch 8: 4.170232\n",
      "Loss on epoch 9: 4.128644\n",
      "Loss on epoch 10: 4.159239\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 614.00 MiB (GPU 0; 4.00 GiB total capacity; 1.97 GiB already allocated; 85.09 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1772/908371517.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Train RNN model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_exs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_exs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_exs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Generate RNN model predictions for test set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1772/1902860091.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;31m# Print accuracy on training and development data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnnModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_exs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_max_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': Accuracy on training set:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnnModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_exs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_max_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1772/2176410615.py\u001b[0m in \u001b[0;36meval_model\u001b[1;34m(model, exs, embeddings_vec, seq_max_len, pred_only)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Run the model on the test examples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_rev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexs_seq_lens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PythonScripts\\JupyterNB\\hw08\\code\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, feats, feats_rev, seq_lens)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mfeats_rev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeats_rev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mlstm2_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats_rev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mlstm2_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm2_outs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# Get hidden vector corresponding to sequence length,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 692\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 614.00 MiB (GPU 0; 4.00 GiB total capacity; 1.97 GiB already allocated; 85.09 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = False\n",
    "bidirectional = True\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a bidirectional LSTM, concatenate the averages of their states\n",
    "\n",
    "Evaluate Two LSTMs (bidirectional) + fully connected network, concatenate the averages of their states.\n",
    "\n",
    "Our accuracy on development data is 77.39%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = True\n",
    "bidirectional = True\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental evaluations on the IMDB dataset.\n",
    "\n",
    "Run the same 4 evaluations on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/imdb/train.txt'\n",
    "dev_path = '../data/imdb/dev.txt'\n",
    "test_path = '../data/imdb/test.txt'\n",
    "\n",
    "test_output_path = 'test-imdb.output.txt'\n",
    "\n",
    "## YOUR CODE HERE\n",
    "word_vecs_path = '../data/glove.6B.300d-relativized.txt'\n",
    "\n",
    "word_vectors = read_word_embeddings(word_vecs_path)\n",
    "word_indexer = word_vectors.word_indexer\n",
    "\n",
    "train_exs = read_and_index_sentiment_examples(train_path, word_indexer)\n",
    "dev_exs = read_and_index_sentiment_examples(dev_path, word_indexer)\n",
    "test_exs = read_and_index_sentiment_examples(blind_test_path, word_indexer)\n",
    "\n",
    "print(repr(len(train_exs)) + \" / \" + \n",
    "      repr(len(dev_exs)) + \" / \" + \n",
    "      repr(len(test_exs)) + \" train / dev / test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "## Using output state of only 1 LSTM\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = False\n",
    "bidirectional = False\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for IMDb dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "## Using output states of 1 LSTM with averaging\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = True\n",
    "bidirectional = False\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for IMDb dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "## Using bidirectional LSTM without averaging state output\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = False\n",
    "bidirectional = True\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for IMDb dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "## Using bidirectional LSTM with averaging state output\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = True\n",
    "bidirectional = True\n",
    "\n",
    "# Train RNN model.\n",
    "model1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for IMDb dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-domain performance\n",
    "\n",
    "Compare the performance of the Bidirectional LSTM with state averaging on the IMDB test set in two scenarios:\n",
    "\n",
    "1. The model is trained on the IMDB training data.\n",
    "\n",
    "2. The model is trained on the Rotten Tomatoes data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "Anything extra goes here. For example:\n",
    "- Train and evaluate each model 10 times, from different random initializations (different seeds). Average the accuracy over the 10 runs and compare the performance of the 4 models on the Rotten Tomatoes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "\n",
    "Include an analysis of the results that you obtained in the experiments above. Also compare with the sentiment classification performance from previous assignments. Show the results using table(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
